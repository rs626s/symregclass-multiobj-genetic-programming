
Running on dataset: Diabetes

Top GP Models:
Model 1:
  Expression: add(sub(0.7141087139670903, ARG6), sub(sub(0.7850814784034406, ARG4), neg(ARG1)))
  Features used: 3
Model 2:
  Expression: neg(neg(sub(ARG9, -0.9634898284031064)))
  Features used: 1
Model 3:
  Expression: neg(sub(ARG1, 0.9003298957815062))
  Features used: 1

Hypothesis Evaluation for Diabetes:
  Accuracy Condition: Failed
  Complexity Condition: Passed
  Hypothesis Not Supported: One or more conditions failed.

Baseline comparisons:
  Decision Tree: MSE = 6506.5491
  Random Forest: MSE = 3365.9377
  Linear Regression: MSE = 2993.0813

Running on dataset: California

Top GP Models:
Model 1:
  Expression: add(add(neg(x0), sub(x0, -0.9349126537203165)), ARG3)
  Features used: 1
Model 2:
  Expression: add(ARG3, ARG3)
  Features used: 1
Model 3:
  Expression: neg(ARG3)
  Features used: 1

Hypothesis Evaluation for California:
  Accuracy Condition: Failed
  Complexity Condition: Passed
  Hypothesis Not Supported: One or more conditions failed.

Baseline comparisons:
  Decision Tree: MSE = 0.6424
  Random Forest: MSE = 0.3056
  Linear Regression: MSE = 0.5494

Running on dataset: Iris

Top GP Models:
Model 1:
  Expression: neg(ARG1)
  Features used: 1
Model 2:
  Expression: neg(ARG3)
  Features used: 1
Model 3:
  Expression: neg(ARG2)
  Features used: 1

Hypothesis Evaluation for Iris:
  Accuracy Condition: Failed
  Complexity Condition: Passed
  Hypothesis Not Supported: One or more conditions failed.

Baseline comparisons:
  Decision Tree: Accuracy = 0.9600
  Random Forest: Accuracy = 0.9667
  Logistic Regression: Accuracy = 0.9733

Running on dataset: Wine

Top GP Models:
Model 1:
  Expression: neg(ARG9)
  Features used: 1
Model 2:
  Expression: neg(ARG12)
  Features used: 1
Model 3:
  Expression: neg(ARG11)
  Features used: 1

Hypothesis Evaluation for Wine:
  Accuracy Condition: Failed
  Complexity Condition: Passed
  Hypothesis Not Supported: One or more conditions failed.

Baseline comparisons:
  Decision Tree: Accuracy = 0.8821
  Random Forest: Accuracy = 0.9665
  Logistic Regression: Accuracy = 0.9611

Running on dataset: Digits

Top GP Models:
Model 1:
  Expression: neg(ARG23)
  Features used: 1
Model 2:
  Expression: neg(ARG16)
  Features used: 1
Model 3:
  Expression: neg(ARG63)
  Features used: 1

Hypothesis Evaluation for Digits:
  Accuracy Condition: Failed
  Complexity Condition: Passed
  Hypothesis Not Supported: One or more conditions failed.

Baseline comparisons:
  Decision Tree: Accuracy = 0.7724
  Random Forest: Accuracy = 0.9433
  Logistic Regression: Accuracy = 0.9143

Running on dataset: BreastCancer

Top GP Models:
Model 1:
  Expression: neg(ARG26)
  Features used: 1
Model 2:
  Expression: neg(ARG15)
  Features used: 1
Model 3:
  Expression: neg(ARG7)
  Features used: 1

Hypothesis Evaluation for BreastCancer:
  Accuracy Condition: Failed
  Complexity Condition: Passed
  Hypothesis Not Supported: One or more conditions failed.

Baseline comparisons:
  Decision Tree: Accuracy = 0.9103
  Random Forest: Accuracy = 0.9667
  Logistic Regression: Accuracy = 0.9526
